{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing regularization approaches\n",
    "\n",
    "Here's a notebook for playing with different penalties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %matplotlib notebook\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'svg'\n",
    "from matplotlib import pyplot as plt\n",
    "from dement import DemEnt\n",
    "import numpy as np\n",
    "from scipy.optimize import minimize, check_grad\n",
    "from scipy.special import erf\n",
    "from scipy.special import expit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize a model\n",
    "--\n",
    "We'll simulate a demographic history that suffers a crash then an exponential recovery\n",
    "\n",
    "Define the time axis $\\mathbf{t}$ (including the boundary at infinity) and the population size trajectory $\\mathbf{y}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = np.array(list(np.arange(0, 10000, 40)))\n",
    "\n",
    "# constant\n",
    "# y_true = 10000 * np.ones(len(t) - 1)\n",
    "\n",
    "# crash followed by exponential growth\n",
    "y_true = 1000 * (4 * np.exp(-t[1:]/100) + 1 + 2 * np.array(t[1:] > 2000, float))\n",
    "\n",
    "# oscillatory\n",
    "# y_true = 1000 * (10 * np.sin(-t[:-1]/100) * ((t[:-1] - t[-2]) / t[-2])**2 + 10)\n",
    "\n",
    "# sigmoid crash at 50 generations ago\n",
    "#y_true = 4000 * (- .95 * expit(-(t[1:] - 50) / 10) + 1) + 1000\n",
    "#y_true = np.concatenate((y_true, [y_true[-1]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "249"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(y_true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The number of sampled haplotypes $n$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 200"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize dement object, and print its docstring:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name '_init_A' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-2d1766dc3f9d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdement\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDemEnt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_true\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdement\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__doc__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mdement\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/work/dement/dement.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, n, t, y, infinite)\u001b[0m\n\u001b[1;32m     49\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mA\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_init_A\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdiff\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbinom_array\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbinom\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name '_init_A' is not defined"
     ]
    }
   ],
   "source": [
    "dement = DemEnt(n, t, y_true)\n",
    "print(dement.__doc__)\n",
    "dement.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inversion\n",
    "--\n",
    "### Initialization with constant model\n",
    "\n",
    "We initialize by fitting a constant population size.\n",
    "According to WSD's scribbles, the MLE assuming $\\eta(t) = \\eta_0$ (constant) is $\\hat \\eta_0 = \\frac{S}{2 H_{n-1}}$, where $S$ is the number of segregating sites (the sum of the observed SFS vector) and $H_{n-1}$ is the $n$th harmonic number.\n",
    "This was derived by using the well-known result (cited in Rosen et al.) that the expected SFS for a constant population is given by $\\xi_i = \\frac{2\\eta_0}{i}$ (in units where $\\eta$ is the population-scaled mutation rate).\n",
    "Then the likelihood for $\\eta_0$ is a Poisson random field parameterized by the $\\xi_i$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "S = dement.sfs.sum()\n",
    "H = (1 / np.arange(1, len(dement.sfs))).sum()\n",
    "y_constant = (S / 2 / H) * np.ones(len(t) - 1)\n",
    "dement.plot(y_constant, y_label='constant MLE')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regularized loss as a penalized log-likelihood\n",
    "We must deal with the asymptotically constant boundary condition.\n",
    "Standard regularizers blow up on the infinite epoch.\n",
    "Let's use half Gaussian instead of Lebesgue measure on time to induce integrability: $\\mathrm{d}\\mu(t) = \\frac{\\sqrt{2}}{\\sqrt{\\pi}\\tau}\\exp\\left(-\\frac{1}{2}\\left(\\frac{t}{\\tau}\\right)^2\\right)\\mathrm{d}t,$\n",
    "where $\\tau$ is the characteristic time to asmptopia (the boundary of our time grid).\n",
    "For example, a modified $L2$ would be\n",
    "$$\n",
    "R\\left[\\eta(t)\\right] = \\int_0^\\infty \\eta(t)^2 \\mathrm{d}\\mu(t) = \\frac{\\sqrt{2}}{\\sqrt{\\pi}\\tau}\\int_0^\\infty \\eta(t)^2 \\exp\\left(-\\frac{1}{2}\\left(\\frac{t}{\\tau}\\right)^2\\right)\\mathrm{d}t.\n",
    "$$\n",
    "So the discretized problem is expressed in terms of the error function $\\DeclareMathOperator{\\erf}{erf}\\erf(\\cdot)$ if $\\eta(t)$ is piecewise constant.\n",
    "This will give extra (but finite) weight / penalty to the infinite epoch based on the survival function of $\\mathrm{d}\\mu(t)$.\n",
    "We can tune how much weight the boundary epoch gets by tuning the width of the Gaussian."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss(y, y_prime, lambda_: float):\n",
    "    '''\n",
    "    negative log likelihood (Poisson random field) and regularizations on divergence from a prior and on the derivative\n",
    "    '''\n",
    "    # gaussian transformed measure\n",
    "    #tau = 100 * dement.t[-2]\n",
    "    #dmu = np.diff(erf(dement.t / tau / np.sqrt(2)))\n",
    "    # Lebesgue on the modeled time interval\n",
    "    dmu = np.diff(t)\n",
    "    # generalized KL divergence (a Bregman divergence)\n",
    "#     R_prior = ((y * np.log(y/y_prime) - y + y_prime) * dmu).sum()\n",
    "    #R_prior = ((y - y_prime)**2 * dmu).sum()\n",
    "#     R_prior = ((y - y_prime)**2).sum()\n",
    "    # L2 on derivative\n",
    "    R_diff = (np.diff(y)**2 * dmu[:-1]).sum()\n",
    "#     y_diff = np.diff(y)\n",
    "#     weight = np.ones(len(y_diff))\n",
    "#     weight[-100:] = 1 + 10 * np.linspace(0, 1, 100)\n",
    "#     R_diff = ((weight * y_diff)**2).sum()\n",
    "    return - dement.ell(y) + lambda_ * R_diff # (R_prior + 1e-2 * R_diff)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Minimize loss with L-BFGS-B\n",
    "\n",
    "I'm using a constant L2 penalty to induce smoothness, and iterating a Bregman from a previous iterate's fit\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initial regularization strength\n",
    "lambda_ = 1e-3\n",
    "\n",
    "# initial and prior set to the constant population MLE\n",
    "y = y_constant\n",
    "y_prime = y_constant\n",
    "\n",
    "for _ in range(3):\n",
    "    result = minimize(loss,\n",
    "                      y_prime,\n",
    "                      args=(y_prime, lambda_),\n",
    "                      # jac=gradF,\n",
    "                      method='L-BFGS-B',\n",
    "                      options=dict(ftol=1e-6, maxfun=np.inf),\n",
    "                      bounds=[(1, None)] * len(y))\n",
    "    assert result.success, result\n",
    "    y = result.x\n",
    "\n",
    "    dement.plot(y, y_label='inferred ($\\lambda = {:.2g}$)'.format(lambda_))    \n",
    "    \n",
    "    # update prior and reduce regularization strength\n",
    "    y_prime = y\n",
    "    lambda_ /= 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y[-101:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cray idea\n",
    "\n",
    "Thm (8) of Rosen et al. tells us that we can always find a piecewise MLE.\n",
    "But we're interested in finding smooth solutions that have the same likelihood as that piecewise MLE.\n",
    "Can we penalize on the value of the MLE—in particular that it equals the optimal piecewise constant one—to learn about the structure of the inverse image?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
