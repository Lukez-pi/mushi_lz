#! /usr/bin/env python
# -*- coding: utf-8 -*-

from dataclasses import dataclass
import numpy as np
from scipy.special import binom
from scipy.stats import poisson
from matplotlib import pyplot as plt
import prox_tv as ptv


@dataclass
class History():
    '''Piecewise constant history. The first epoch starts at zero, and the last
    epoch extends to infinity. Can be used for η or μ.

    change_points: epoch change points (times)
    vals: constant values for each epoch (rows)
    '''
    change_points: np.array
    vals: np.ndarray

    def __post_init__(self):
        if any(np.diff(self.change_points) <= 0) or any(
           np.isinf(self.change_points)) or any(self.change_points <= 0):
            raise ValueError('change_points must be increasing, finite, and '
                             'positive')
        if len(self.vals) != len(self.change_points) + 1:
            raise ValueError(f'len(change_points) = {len(self.change_points)}'
                             f' implies {len(self.change_points) + 1} epochs,'
                             f' but len(vals) = {len(self.vals)}')
        if np.any(self.vals <= 0) or np.sum(np.isinf(self.vals)):
            raise ValueError('elements of vals must be finite and positive')
        self.m = len(self.vals)

    def plot(self, i=None, **kwargs) -> None:
        '''plot the history

        i: value column to plot (optional)
        kwargs: key word arguments passed to plt.step
        '''
        t = np.concatenate((np.array([0]), self.change_points))
        if i is not None:
            vals = self.vals[:, i]
        else:
            vals = self.vals
        plt.step(t, vals, where='post', **kwargs)
        if 'label' in kwargs:
            plt.legend()

    def arrays(self):
        '''return time grid and values in each cell
        '''
        t = np.concatenate((np.array([0]),
                            self.change_points,
                            np.array([np.inf])))
        return t, self.vals

    def epochs(self):
        '''generator yielding epochs as tuples: (start_time, end_time, value)
        '''
        for i in range(self.m):
            if i == 0:
                start_time = 0
            else:
                start_time = self.change_points[i - 1]
            if i == self.m - 1:
                end_time = np.inf
            else:
                end_time = self.change_points[i]
            value = self.vals[i]
            yield (start_time, end_time, value)

    def check_grid(self, other: int):
        if any(self.change_points != other.change_points):
            return False
        else:
            return True


class kSFS():
    '''The kSFS model described in the text
    '''

    def __init__(self, η: History, X: np.ndarray = None, n: int = None):
        '''Sample frequency spectrum

        η: demographic history
        X: observed k-SFS matrix (optional)
        n: number of haplotypes (optional)
        '''
        self.η = η
        if X is not None:
            self.X = X
            self.n = len(X) + 1
        elif not n:
            raise ValueError('either x or n must be specified')
        else:
            self.n = n
        self.L = kSFS.C(self.n) @ kSFS.M(self.n, self.η)

    @staticmethod
    def C(n: int) -> np.ndarray:
        '''The C matrix defined in the text

        n: number of sampled haplotypes
        '''
        W1 = np.zeros((n - 1, n - 1))
        W2 = np.zeros((n - 1, n - 1))
        b = np.arange(1, n - 1 + 1)
        # j = 2
        W1[:, 0] = 6 / (n + 1)
        W2[:, 0] = 0
        # j = 3
        W1[:, 1] = 10 * (5 * n - 6 * b - 4) / (n + 2) / (n + 1)
        W2[:, 1] = (20 * (n - 2)) / (n+1) / (n+2)
        for j in range(2, n - 1):
            col = j - 2
            # procedurally generated by Zeilberger's algorithm in Mathematica
            W1[:, col + 2] = -((-((-1 + j)*(1 + j)**2*(3 + 2*j)*(j - n)*(4 + 2*j - 2*b*j + j**2 - b*j**2 + 4*n + 2*j*n + j**2*n)*W1[:, col]) - (-1 + 2*j)*(3 + 2*j)*(-4*j - 12*b*j - 4*b**2*j - 6*j**2 - 12*b*j**2 - 2*b**2*j**2 - 4*j**3 + 4*b**2*j**3 - 2*j**4 + 2*b**2*j**4 + 4*n + 2*j*n - 6*b*j*n + j**2*n - 9*b*j**2*n - 2*j**3*n - 6*b*j**3*n - j**4*n - 3*b*j**4*n + 4*n**2 + 6*j*n**2 + 7*j**2*n**2 + 2*j**3*n**2 + j**4*n**2)*W1[:, col + 1])/(j**2*(2 + j)*(-1 + 2*j)*(1 + j + n)*(3 + b + j**2 - b*j**2 + 3*n + j**2*n)))
            W2[:, col + 2] = ((-1 + j)*(1 + j)*(2 + j)*(3 + 2*j)*(j - n)*(1 + j - n)*(1 + j + n)*W2[:, col] + (-1 + 2*j)*(3 + 2*j)*(1 + j - n)*(j + n)*(2 - j - 2*b*j - j**2 - 2*b*j**2 + 2*n + j*n + j**2*n)*W2[:, col + 1])/((-1 + j)*j*(2 + j)*(-1 + 2*j)*(j - n)*(j + n)*(1 + j + n))

        return W1 - W2

    @staticmethod
    def M(n: int, η: History) -> np.ndarray:
        '''The M matrix defined in the text

        n: number of sampled haplotypes
        η: demographic history
        '''
        t, y = η.arrays()
        # epoch durations
        s = np.diff(t)
        u = np.exp(-s / y)
        u = np.concatenate((np.array([1]), u))

        binom_vec = binom(np.arange(2, n + 1), 2)

        return np.diag(1 / binom_vec) \
            @ np.cumprod(u[np.newaxis, :-1],
                         axis=1) ** binom_vec[:, np.newaxis] \
            @ (np.eye(len(y), k=0) - np.eye(len(y), k=-1)) \
            @ np.diag(y)

    def tmrca_cdf(self) -> np.ndarray:
        '''The cdf of the TMRCA of at each change point
        '''
        t, y = self.η.arrays()
        # epoch durations
        s = np.diff(t)
        u = np.exp(-s / y)
        u = np.concatenate((np.array([1]), u))
        # the A_2j are the product of this matrix
        # NOTE: using letter  "l" as a variable name to match text
        l = np.arange(2, self.n + 1)[:, np.newaxis]
        with np.errstate(divide='ignore'):
            A2_terms = l * (l-1) / (l * (l-1) - l.T * (l.T-1))
        np.fill_diagonal(A2_terms, 1)
        A2 = np.prod(A2_terms, axis=0)

        return 1 - (A2[np.newaxis, :]
                    @ np.cumprod(u[np.newaxis, 1:-1],
                                 axis=1) ** binom(np.arange(2, self.n + 1), 2
                                                  )[:, np.newaxis]).T

    def simulate(self, μ: History, seed: int = None) -> None:
        '''simulate a SFS under the Poisson random field model (no linkage)
        assigns simulated SFS to self.X

        μ: mutation spectrum history
        seed: random seed (optional)
        '''
        if not self.η.check_grid(μ):
            raise ValueError('η and μ histories must use the same time grid')
        np.random.seed(seed)
        self.X = poisson.rvs(self.L @ μ.vals)

    def ℓ(self, μ: History, grad: bool = False) -> np.float:
        '''Poisson random field log-likelihood of history

        μ: mutation spectrum history
        grad: flag to return gradient wrt μ
        '''
        if self.X is None:
            raise ValueError('use simulate() to generate data first')
        Ξ = self.L @ μ.vals
        if grad:
            dℓdμ = self.L.T @ (self.X / Ξ - 1)
            return dℓdμ
        else:
            return poisson.logpmf(self.X, Ξ).sum(axis=0)

    def d_kl(self, μ: History, grad: bool = False) -> float:
        '''Kullback-Liebler divergence between normalized SFS and its
        expectation under history
        ignores constant term

        μ: mutation spectrum history
        grad: flag to return gradient wrt μ
        '''
        if self.X is None:
            raise ValueError('use simulate() to generate data first')
        X_normalized = self.X / self.X.sum(axis=0)
        Ξ = self.L @ μ.vals
        Ξ_normalized = Ξ / Ξ.sum(axis=1)
        if grad:
            return -self.L.T @ ((X_normalized / Ξ) * (1 - Ξ_normalized))
        else:
            return (-X_normalized @ np.log(Ξ_normalized)).sum(axis=0)

    def lsq(self, μ: History, grad: bool = False) -> float:
        '''least-squares loss between SFS and its expectation under history

        μ: mutation spectrum history
        grad: flag to return gradient wrt μ
        '''
        if self.X is None:
            raise ValueError('use simulate() to generate data first')
        Ξ = self.L @ μ.vals
        if grad:
            return self.L.T @ (Ξ - self.X)
        else:
            return (1 / 2) * ((Ξ - self.X) ** 2).sum(axis=0)

    def constant_μ_MLE(self) -> History:
        '''gives the MLE for a constant μ history
        '''
        if self.X is None:
            raise ValueError('use simulate() to generate data first')
        z0 = self.X.sum(axis=0) / np.sum(self.L)
        return History(self.η.change_points,
                       z0[np.newaxis, :] * np.ones((self.η.m, 1)))

    def infer_μ(self, λ: np.float64 = 0, α: np.float64 = .99,
                s: np.float64 = .01, steps: int = 100, fit='prf',
                bins: np.ndarray = None) -> History:
        '''return inferred μ history given the sfs and η history

        λ: regularization strength
        α: relative penalty on L1 vs L2
        s: step size parameter for proximal gradient descent
        steps: number of proximal gradient descent steps
        fit: fit function, 'prf' for Poisson random field, 'kl' for
             Kullback-Leibler divergence, 'lsq' for least-squares
        '''
        assert λ >= 0, 'λ must be nonnegative'
        assert 0 <= α <= 1, 'α must be in the unit interval'
        self.bins = bins
        if bins is not None:
            bin_idxs = np.digitize(np.arange(self.n - 1), bins=bins)
            X_binned = np.zeros((len(bins), self.X.shape[1]))
            L_binned = np.zeros((len(bins), self.η.m))
            for col in range(self.X.shape[1]):
                X_binned[:, col] = np.bincount(bin_idxs,
                                               weights=self.X[:, col])
            for col in range(self.η.m):
                L_binned[:, col] = np.bincount(bin_idxs,
                                               weights=self.L[:, col])
            # stash the unbinned variables
            X_true = self.X
            L_true = self.L
            # temporarily update instance variables to the binned ones
            self.X = X_binned
            self.L = L_binned
        D = (np.eye(self.η.m, k=0) - np.eye(self.η.m, k=-1))
        W = np.eye(self.η.m)
        W[0, 0] = 0
        D2 = D.T @ W @ D
        # gradient of differentiable piece of loss function
        if fit == 'prf':
            def misfit_func(*args, **kwargs):
                return -self.ℓ(*args, **kwargs)
        elif fit == 'kl':
            misfit_func = self.d_kl
        elif fit == 'lsq':
            misfit_func = self.lsq
        else:
            raise ValueError(f'unrecognized fit argument {fit}')
        # initialize using constant μ history MLE
        μ = self.constant_μ_MLE()
        for _ in range(steps):
            Z = μ.vals
            G = misfit_func(μ, grad=True) + λ * (1 - α) * D2 @ Z
            if not np.all(np.isfinite(G)):
                raise RuntimeError(f'invalid gradient: {G}')
            Z = Z - s * G
            # L1 prox operator on row dimension (oddly 1-based indexed in
            # proxtv) with weight λα
            Z = ptv.tvgen(Z, [λ * α], [1], [1])
            Z = np.clip(Z, 1e-6, np.inf)
            if not np.all(np.isfinite(Z)):
                raise RuntimeError(f'invalid z value: {Z}')
            μ.vals = Z
        if bins is not None:
            # restore stashed unbinned variables
            self.X = X_true
            self.L = L_true
        return μ

    def plot(self, i: int = None, μ: History = None, prf_quantiles=False):
        '''plot the SFS data and optionally the expected SFS given μ

        i: component i of kSFS (default first)
        μ: mutation intensity history (optional)
        prf_quantiles: if True show 95% marginal intervals using the Poisson
                       random field
        '''
        if i is None:
            i = 0
        if μ is not None:
            z = μ.vals[:, i]
            ξ = self.L @ z
            if self.bins is not None:
                for bin in self.bins:
                    plt.axvline(bin, c='k', ls=':', alpha=0.2)
            plt.plot(range(1, self.n), ξ, 'r--', label=r'$\xi$')
            if prf_quantiles:
                ξ_lower = poisson.ppf(.025, ξ)
                ξ_upper = poisson.ppf(.975, ξ)
                plt.fill_between(range(1, self.n),
                                 ξ_lower, ξ_upper,
                                 facecolor='r', alpha=0.25,
                                 label='inner 95%\nquantile')
        plt.plot(range(1, len(self.X) + 1), self.X[:, i],
                 'k.', alpha=.25, label=r'data')
        plt.xlabel('$b$')
        plt.ylabel(r'$ξ_b$')
        plt.xscale('log')
        plt.yscale('symlog')
