#! /usr/bin/env python
# -*- coding: utf-8 -*-

from dataclasses import dataclass
import numpy as np
from scipy.special import binom
from functools import lru_cache
from scipy.stats import poisson
from matplotlib import pyplot as plt
from scipy.optimize import minimize


@dataclass(frozen=True)
class PiecewiseConstantHistory():
    '''The first epoch starts at zero, and the last epoch extends to infinity.
    Can be used for η or μ

    change_points: epoch change points (times)
    vals: vector of constant values for each epoch
    '''
    change_points: np.array
    vals: np.ndarray

    def __post_init__(self):
        if any(np.diff(self.change_points) <= 0) or any(
           np.isinf(self.change_points)) or any(self.change_points <= 0):
            raise ValueError('change_points must be increasing, finite, and '
                             'positive')
        if len(self.vals) != len(self.change_points) + 1:
            raise ValueError(f'len(change_points) = {len(self.change_points)}'
                             f' implies {len(self.change_points) + 1} epochs,'
                             f' but len(vals) = {len(self.vals)}')
        if any(self.vals <= 0) or np.isinf(self.vals).sum():
            raise ValueError('elements of vals must be finite and positive')

    def m(self):
        '''number of epochs
        '''
        return len(self.vals)

    def __hash__(self):
        '''needed for hashability
        '''
        return hash((tuple(self.change_points), tuple(self.vals)))

    def __eq__(self, other) -> bool:
        '''needed for hashability
        '''
        if any(self.change_points != other.change_points):
            return False
        if any(self.vals != other.vals):
            return False
        return True

    def plot(self, **kwargs) -> None:
        '''plot the history

        kwargs: key word arguments passed to plt.step
        '''
        t = np.insert(self.change_points, 0, 0)
        plt.step(t, self.vals, where='post', **kwargs)
        if 'label' in kwargs:
            plt.legend()


@dataclass()
class JointHistory():
    '''Piecewise constant history of population size η and mutation rate μ.
    both histories must use the same time grid

    η: effective population size history
    μ: mutation rate history
    '''
    η: PiecewiseConstantHistory
    μ: PiecewiseConstantHistory

    def __post_init__(self):
        if any(self.η.change_points != self.μ.change_points):
            raise ValueError('η and μ histories must use the same time grid')

    def plot(self, **kwargs) -> np.ndarray:
        '''plot the history

        kwargs: keyword arguments passed to plotting calls
        pass fig=<plt.figure object> to add to current axes
        '''
        fig = kwargs.pop('fig', None)
        if fig:
            axes = fig.axes
        else:
            fig, axes = plt.subplots(2, 1, sharex=True, figsize=(6, 6))
        plt.sca(axes[0])
        self.η.plot(**kwargs)
        plt.ylabel('$η(t)$')

        plt.sca(axes[1])
        self.μ.plot(**kwargs)
        plt.xlabel('$t$')
        plt.xscale('symlog')
        plt.ylabel('$μ(t)$')
        plt.ylim([0, None])

        return fig


class SFS():
    '''The SFS model described in the text
    '''

    def __init__(self, n: int = None, x: np.ndarray = None):
        '''pass one of these arguments

        n: number of sampled haplotypes
        sfs: observed sfs vector
        '''
        if x is not None:
            self.x = x
            self.n = len(x) + 1
            assert n is None, 'pass only one of n or x'
        elif n is not None:
            self.x = None
            self.n = n
            assert x is None, 'pass only one of n or x'
        else:
            raise ValueError('must pass either n or x')
        if self.n < 2:
            raise ValueError('n must be larger than 1')
        self._binom_vec = binom(np.arange(2, self.n + 1), 2)
        self._binom_array_recip = np.diag(1 / self._binom_vec)
        self.C = SFS.C(self.n)

    @staticmethod
    def W1(n: int) -> np.ndarray:
        '''The W1 matrix defined in the text

        n: number of sampled haplotypes
        '''
        W1 = np.zeros((n - 1, n - 1))
        b = np.arange(1, n - 1 + 1)
        # j = 2
        W1[:, 0] = 6 / (n + 1)
        # j = 3
        W1[:, 1] = 10 * (5 * n - 6 * b - 4) / (n + 2) / (n + 1)
        for j in range(2, n - 1):
            col = j - 2
            # procedurally generated by Zeilberger's algorithm in Mathematica
            W1[:, col + 2] = -((-((-1 + j)*(1 + j)**2*(3 + 2*j)*(j - n)*(4 + 2*j - 2*b*j + j**2 - b*j**2 + 4*n + 2*j*n + j**2*n)*W1[:, col]) - (-1 + 2*j)*(3 + 2*j)*(-4*j - 12*b*j - 4*b**2*j - 6*j**2 - 12*b*j**2 - 2*b**2*j**2 - 4*j**3 + 4*b**2*j**3 - 2*j**4 + 2*b**2*j**4 + 4*n + 2*j*n - 6*b*j*n + j**2*n - 9*b*j**2*n - 2*j**3*n - 6*b*j**3*n - j**4*n - 3*b*j**4*n + 4*n**2 + 6*j*n**2 + 7*j**2*n**2 + 2*j**3*n**2 + j**4*n**2)*W1[:, col + 1])/(j**2*(2 + j)*(-1 + 2*j)*(1 + j + n)*(3 + b + j**2 - b*j**2 + 3*n + j**2*n)))
        return W1

    @staticmethod
    def W2(n: int) -> np.ndarray:
        '''The W2 matrix defined in the text

        n: number of sampled haplotypes
        '''
        W2 = np.zeros((n - 1, n - 1))
        b = np.arange(1, n - 1 + 1)
        # j = 2
        W2[:, 0] = 0
        # j = 3
        W2[:, 1] = (20 * (n - 2)) / (n+1) / (n+2)
        for j in range(2, n - 1):
            col = j - 2
            # procedurally generated by Zeilberger's algorithm in Mathematica
            W2[:, col + 2] = ((-1 + j)*(1 + j)*(2 + j)*(3 + 2*j)*(j - n)*(1 + j - n)*(1 + j + n)*W2[:, col] + (-1 + 2*j)*(3 + 2*j)*(1 + j - n)*(j + n)*(2 - j - 2*b*j - j**2 - 2*b*j**2 + 2*n + j*n + j**2*n)*W2[:, col + 1])/((-1 + j)*j*(2 + j)*(-1 + 2*j)*(j - n)*(j + n)*(1 + j + n))
        return W2

    @staticmethod
    def C(n: int) -> np.ndarray:
        '''The C matrix defined in the text

        n: number of sampled haplotypes
        '''
        return SFS.W1(n) - SFS.W2(n)

    @lru_cache(maxsize=1)
    def M(self, η: PiecewiseConstantHistory) -> np.ndarray:
        '''The M matrix defined in the text

        η: η history
        '''
        t = np.concatenate(([0], η.change_points, [np.inf]))
        # epoch durations
        s = np.diff(t)
        u = np.exp(-s / η.vals)
        u = np.insert(u, 0, 1)

        return self._binom_array_recip \
               @ np.cumprod(u[np.newaxis, :-1],
                            axis=1) ** self._binom_vec[:, np.newaxis] \
               @ (np.eye(η.m(), k=0) - np.eye(η.m(), k=-1)) \
               @ np.diag(η.vals)

    @lru_cache(maxsize=1)
    def Γ(self, η: PiecewiseConstantHistory,
          h: np.float = 1) -> np.ndarray:
        '''The Gamma matrix defined in the text

        η: η history
        h: relative increase in penalty as we approach coalescent horizon
        '''
        t = np.concatenate(([0], η.change_points, [np.inf]))
        # epoch durations
        s = np.diff(t)
        u = np.exp(-s / η.vals)
        u = np.insert(u, 0, 1)

        # the A_2j product of these terms
        # NOTE: using letter  "l" as a variable name to match text
        l = np.array(range(2, self.n + 1))[:, np.newaxis]
        with np.errstate(divide='ignore'):
            A2_terms = l * (l-1) / (l * (l-1) - l.T * (l.T-1))
        np.fill_diagonal(A2_terms, 1)
        A2 = np.prod(A2_terms, axis=0)

        return (np.diag(np.insert(h * np.ones(η.m() - 1), 0, h - 1)) + (1 - h)
                * np.diagflat(A2[np.newaxis, :]
                              @ np.cumprod(u[np.newaxis, :-1],
                                           axis=1)
                              ** self._binom_vec[:, np.newaxis])) \
            @ (np.eye(η.m(), k=0) - np.eye(η.m(), k=-1))

    def ξ(self, history: JointHistory, grad_μ: bool = False) -> np.ndarray:
        '''expected sfs vector

        history: η and μ joint history
        grad_μ: flag to return jacobian wrt history.μ
        '''
        z = history.μ.vals
        CM = self.C @ self.M(history.η)
        if grad_μ:
            return CM @ z, CM
        return CM @ z

    def simulate(self, history: JointHistory) -> None:
        '''simulate a SFS under the Poisson random field model (no linkage)

        history: η and μ joint history
        '''
        self.x = poisson.rvs(self.ξ(history))

    def ℓ(self, history: JointHistory, grad_μ: bool = False) -> np.float:
        '''Poisson random field log-likelihood of history

        history: η and μ joint history
        grad_μ: flag to return gradient wrt history.μ
        '''
        if self.x is None:
            raise ValueError('use simulate() to generate data first')
        if grad_μ:
            ξ, J_μξ = self.ξ(history, grad_μ=True)
            ℓ = poisson.logpmf(self.x, ξ).sum()
            dℓdμ = (((self.x / ξ)[:, np.newaxis] - 1) * J_μξ).sum(axis=0)
            return ℓ, dℓdμ
        else:
            return poisson.logpmf(self.x, self.ξ(history)).sum()

    def constant_μ_MLE(self, η: PiecewiseConstantHistory
                       ) -> PiecewiseConstantHistory:
        '''gives the MLE for a constant μ history

        η: η history
        '''
        if self.x is None:
            raise ValueError('use simulate() to generate data first')
        z0 = (self.x.sum() / (SFS.C(self.n) @ self.M(η)).sum())
        return PiecewiseConstantHistory(η.change_points, z0 * np.ones(η.m()))

    def ρ_η(self, history: JointHistory, h: np.float = 1,
            grad_η: bool = False):
        '''η regulizer described in the text

        history: η and μ joint history
        h: relative increase in penalty as we approach coalescent horizon
        grad_η: flag to return gradient wrt history.η
        '''
        Γ = self.Γ(history.η, h)
        y = history.η.vals
        ρ_η = ((Γ @ y)**2).sum()
        if grad_η:
            raise NotImplementedError('gradient of ρ_η not implemented')
        return ρ_η

    def ρ_μ(self, history: JointHistory, h: np.float = 1,
            grad_μ: bool = False):
        '''μ regulizer described in the text

        history: η and μ joint history
        h: relative increase in penalty as we approach coalescent horizon
        grad_μ: flag to return gradient wrt history.μ
        '''
        Γ = self.Γ(history.η, h)
        z = history.μ.vals
        ρ_μ = ((Γ @ z)**2).sum()
        if grad_μ:
            dρ_μdμ = 2 * Γ.T @ Γ @ z
            return ρ_μ, dρ_μdμ
        return ρ_μ

    def L(self, history: JointHistory,
          λ_η: np.float = 0, λ_μ: np.float = 0,
          h: np.float = 1, grad_μ: bool = False) -> np.float:
        '''loss: negative log likelihood (Poisson random field) and
        regularization as described in the text

        history: η and μ joint history
        λ_η: regularization strength on η
        λ_μ: regularization strength on μ
        h: relative increase in penalty as we approach coalescent horizon
        grad_μ: flag to return gradient wrt history.μ
        '''
        ρ_η = self.ρ_η(history, h)
        if grad_μ:
            ρ_μ, dρ_μdμ = self.ρ_μ(history, h, grad_μ=True)
            ℓ, dℓdμ = self.ℓ(history, grad_μ=True)
            L = -ℓ + λ_η * ρ_η + λ_μ * ρ_μ
            dLdμ = -dℓdμ + λ_μ * dρ_μdμ
            return L, dLdμ
        ρ_μ = self.ρ_μ(history, h)
        return -self.ℓ(history) + λ_η * ρ_η + λ_μ * ρ_μ

    def infer_μ(self,
                η: PiecewiseConstantHistory = None,
                λ_μ: np.float = 0,
                h: np.float = 1) -> PiecewiseConstantHistory:
        '''infer the μ history given the simulated sfs and η history

        η: η history
        λ_μ: regularization strength
        h: relative increase in penalty as we approach coalescent horizon
        '''
        # function to minimize
        def f(z):
            history = JointHistory(η,
                                   PiecewiseConstantHistory(η.change_points,
                                                            z))
            return self.L(history, λ_η=0, λ_μ=λ_μ, h=h, grad_μ=True)
        result = minimize(f,
                          self.constant_μ_MLE(η).vals,
                          jac=True,
                          method='L-BFGS-B',
                          options=dict(
                                       # ftol=1e-10,
                                       maxfun=np.inf,
                                       maxiter=np.inf
                                       ),
                          bounds=[(1e-10, None)] * η.m()
                          )
        if not result.success:
            print(result.message)

        return JointHistory(η, PiecewiseConstantHistory(η.change_points,
                                                        result.x))

    def plot(self, history: JointHistory = None):
        '''plot the SFS data and optionally the expected SFS under history

        history: joint η and μ history
        '''
        if history is not None:
            ξ = self.ξ(history)
            ξ_lower = poisson.ppf(.025, ξ)
            ξ_upper = poisson.ppf(.975, ξ)
            plt.plot(range(1, self.n), ξ, 'r--', label=r'$\xi$')
            plt.fill_between(range(1, self.n),
                             ξ_lower, ξ_upper,
                             facecolor='r', alpha=0.25,
                             label='inner 95%\nquantile')
        plt.plot(range(1, len(self.x) + 1), self.x,
                 'k.', alpha=.25, label=r'data')
        plt.xlabel('$b$')
        plt.ylabel(r'$ξ_b$')
        plt.xscale('log')
        plt.yscale('symlog')
        plt.legend()
